# -*- encoding: utf-8 -*-
import faiss
import numpy as np
import net
import torch
import yaml_utils as Yaml
import utils
from imutils import paths
from face_alignment import align

class AdaFaceFeature:
    """
    äººè„¸ç‰¹å¾å€¼é¢„æµ‹
    """
    __instance = None
    
    def __new__(cls, *args, **kwargs):

        if cls.__instance is None:
            cls.__instance = super().__new__(cls)
        return cls.__instance

    def __init__(self,file_name="config/config.yaml") -> None:
        """
        åˆå§‹åŒ–é…ç½®
        """
        self.config = Yaml.get_yaml_config(file_name)
        self.adaface_config = self.config['adaface']['zero']
        self.adaface_models = {self.adaface_config['model']: self.adaface_config['model_file'],}
        pass

    def load_pretrained_model(self):
        """
        åŠ è½½æ¨¡å‹
        """
        
        # load model and pretrained statedict
        architecture = self.adaface_config['model']
        assert architecture in self.adaface_models.keys()
        model = net.build_model(architecture)
        statedict = torch.load(
            self.adaface_models[architecture], map_location=torch.device('cpu'))['state_dict']
        model_statedict = {key[6:]: val for key,
                           val in statedict.items() if key.startswith('model.')}
        model.load_state_dict(model_statedict)
        model.eval()
        self.model = model
        return self



    def to_input(self,pil_rgb_image):
        """
        PIL RGBå›¾åƒå¯¹è±¡è½¬æ¢ä¸ºPyTorchæ¨¡å‹çš„è¾“å…¥å¼ é‡
        """
        tensor = None
        try:
            np_img = np.array(pil_rgb_image)
            brg_img = ((np_img[:, :, ::-1] / 255.) - 0.5) / 0.5
            #tensor = torch.tensor([brg_img.transpose(2, 0,1)]).float()
            tensor = torch.tensor(np.array([brg_img.transpose(2, 0,1)])).float()
        except Exception :
            return tensor    
        return tensor




    def b64_get_represent(self,path):
        """
        è·å–è„¸éƒ¨ç‰¹å¾å‘é‡
        """
        
        feature = None
        
        aligned_rgb_img =  utils.get_base64_to_Image(path).convert('RGB')
        bgr_tensor_input = self.to_input(aligned_rgb_img)
        if bgr_tensor_input is not None:
            feature, _ = self.model(bgr_tensor_input)
        else:
           print(f"æ— æ³•æå–è„¸éƒ¨ç‰¹å¾å‘é‡ ğŸ¥·ğŸ¥·ğŸ¥·")     
        return feature
    

    def byte_get_represent(self,path):
        """
        è·å–è„¸éƒ¨ç‰¹å¾å‘é‡
        """
        
        feature = None
        
        aligned_rgb_img =  utils.get_byte_to_Image(path).convert('RGB')
        bgr_tensor_input = self.to_input(aligned_rgb_img)
        if bgr_tensor_input is not None:
            feature, _ = self.model(bgr_tensor_input)
        else:
           print(f"æ— æ³•æå–è„¸éƒ¨ç‰¹å¾å‘é‡ ğŸ¥·ğŸ¥·ğŸ¥·")     
        return feature
    

    def findCosineDistance(self,source_representation, test_representation):
        """
        è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦å¾—åˆ†
        """
        import torch.nn.functional as F
        return F.cosine_similarity(source_representation, test_representation)

class FaceDatabase:
    def __init__(self, dimension=512):
        """
        åˆå§‹åŒ– FaceDatabase ç±»
        Parameters:
        dimension (int): äººè„¸å‘é‡çš„ç»´åº¦ï¼Œé»˜è®¤ä¸º 512
        """
        self.dimension = dimension  # ä¿å­˜äººè„¸å‘é‡çš„ç»´åº¦
        self.index = faiss.IndexFlatIP(dimension)  # åˆ›å»º Faiss çš„ä½™å¼¦ç›¸ä¼¼åº¦ç´¢å¼•
        self.ids = []  # ä¿å­˜äººè„¸å‘é‡å¯¹åº”çš„ id

    def addFace(self, face_id, face_vector):
        """
        å°†äººè„¸å‘é‡æ·»åŠ è¿›æ•°æ®åº“
        Parameters:
        face_id: äººè„¸ id
        face_vector: äººè„¸å‘é‡
        """
        if face_vector.shape[0] != self.dimension:
            raise ValueError("Face vector dimension does not match the database dimension")  # æŠ›å‡ºç»´åº¦ä¸åŒ¹é…çš„å¼‚å¸¸
        self.index.add(np.expand_dims(face_vector, axis=0))  # å‘ Faiss ç´¢å¼•ä¸­æ·»åŠ äººè„¸å‘é‡
        self.ids.append(face_id)  # è®°å½•äººè„¸å‘é‡çš„ id

    def searchSimilarFaces(self, query_vector, threshold):
        """
        æŸ¥è¯¢ç›¸ä¼¼çš„äººè„¸å‘é‡
        Parameters:
        query_vector: æŸ¥è¯¢å‘é‡
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        Returns:
        tuple or None: è¿”å›è¶…è¿‡é˜ˆå€¼çš„æœ€ç›¸ä¼¼äººè„¸å‘é‡å’Œå¯¹åº”çš„è·ç¦»ï¼Œå¦‚æœæ²¡æœ‰è¶…è¿‡é˜ˆå€¼çš„ï¼Œåˆ™è¿”å› None
        """
        distances, indices = self.index.search(np.expand_dims(query_vector, axis=0), len(self.ids))  # ä½¿ç”¨ Faiss è¿›è¡Œæœç´¢4
        print(distances)
        similar_faces = [(self.ids[i], distances[0][i]) for i in range(len(self.ids)) if distances[0][i] > threshold]  # ä¿å­˜è¶…è¿‡é˜ˆå€¼çš„æœ€ç›¸ä¼¼äººè„¸å‘é‡
        if similar_faces:
            most_similar_face = max(similar_faces, key=lambda x: x[1])  # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„äººè„¸
            return most_similar_face  # è¿”å›æœ€ç›¸ä¼¼çš„äººè„¸å’Œå¯¹åº”çš„è·ç¦»
        else:
            return None  # å¦‚æœæ²¡æœ‰è¶…è¿‡é˜ˆå€¼çš„åˆ™è¿”å› None

    def removeFaceById(self, face_id):
        """
        æ ¹æ®äººè„¸ id åˆ é™¤æ•°æ®åº“å†…çš„äººè„¸å‘é‡
        Parameters:
        face_id: å¾…åˆ é™¤çš„äººè„¸ id
        """
        #å…ˆåˆ é™¤é«˜ç´¢å¼•çš„å…ƒç´ ï¼Œå†åˆ é™¤ä½ç´¢å¼•çš„å…ƒç´ ï¼Œé¿å…ç´¢å¼•é”™ä½çš„é—®é¢˜ã€‚
        index_to_remove = [i for i, stored_id in enumerate(self.ids) if stored_id == face_id]  # æ‰¾åˆ°éœ€è¦åˆ é™¤çš„äººè„¸å‘é‡çš„ç´¢å¼•
        for i in sorted(index_to_remove, reverse=True):
            self.index.remove_ids(np.array([i]))  # ä» Faiss ç´¢å¼•ä¸­åˆ é™¤å¯¹åº”çš„äººè„¸å‘é‡
            del self.ids[i]  # ä» id åˆ—è¡¨ä¸­åˆ é™¤å¯¹åº”çš„ id

    def getNumFaces(self):
        """
        è·å–æ•°æ®åº“å†…å­˜å‚¨çš„äººè„¸å‘é‡æ•°é‡
        Returns:
        int: äººè„¸å‘é‡æ•°é‡
        """
        return len(self.ids)  # è¿”å›ä¿å­˜çš„äººè„¸å‘é‡æ•°é‡

    def clearDatabase(self):
        """
        æ¸…ç©ºæ•°æ®åº“ï¼Œåˆ é™¤æ‰€æœ‰çš„äººè„¸å‘é‡
        """
        self.index.reset()  # é‡ç½® Faiss ç´¢å¼•
        self.ids.clear()  # æ¸…ç©º id åˆ—è¡¨    

adaface_models = {
    'ir_18':"models/adaface_ir18_webface4m.ckpt",
}

def load_pretrained_model(architecture='ir_18'):
    # load model and pretrained statedict
    assert architecture in adaface_models.keys()
    model = net.build_model(architecture)
    statedict = torch.load(adaface_models[architecture])['state_dict']
    model_statedict = {key[6:]:val for key, val in statedict.items() if key.startswith('model.')}
    model.load_state_dict(model_statedict)
    model.eval()
    return model

def to_input(pil_rgb_image):
    np_img = np.array(pil_rgb_image)
    brg_img = ((np_img[:,:,::-1] / 255.) - 0.5) / 0.5
    tensor = torch.tensor([brg_img.transpose(2,0,1)]).float()
    return tensor

if __name__ == '__main__':
    # åˆ›å»ºä¸€ä¸ª FaceDatabase å®ä¾‹
    face_db = FaceDatabase()
    print("è·å–ç‰¹å¾å¼€å§‹")
    adaface =  AdaFaceFeature()
    adaface.load_pretrained_model()
    #å¤„ç†è¿‡çš„äººè„¸ç…§ç‰‡ï¼Œä»¥åšé¢éƒ¨å¯¹é½å¤„ç†,å¤§å°ï¼š 112*112
    dir_path = "./mtcnn/"
    #byte æµ‹è¯•
    for img_path in paths.list_images(dir_path):
        feature = adaface.byte_get_represent(utils.get_image_path_to_byte(img_path))
        #print("feature = ",feature.detach().numpy()[0])
        #print("feature.shape = ",len(feature.detach().numpy()[0]))
        face_db.addFace(img_path, feature.detach().numpy()[0]) 
        #print(utils.feature2json(feature))
        #print(feature[0].size())
    
    #äººè„¸éªŒè¯
    query_vector, norm = adaface(torch.randn(2,3,112,112))
    
    test_path = "./test/img1.jpg"
    aligned_rgb_img = align.get_aligned_face(test_path)
    bgr_tensor_input = to_input(aligned_rgb_img)
    query_vector, _ = adaface(bgr_tensor_input)
    #query_vector = adaface.byte_get_represent(utils.get_image_path_to_byte())
    
    
    threshold = -1000000000000000000
    result = face_db.searchSimilarFaces(query_vector.detach().numpy()[0], threshold)
    if result:
        similar_id, distance = result
        print(f"The most similar face is {similar_id} with distance {distance}")
    else:
        print("No similar face found")